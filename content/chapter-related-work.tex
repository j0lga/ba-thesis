% !TEX root = ../my-thesis.tex
%
\chapter{Related Work}
\label{sec:related}

\cleanchapterquote{Speech 100}{Olga Yakobson}{(Ph.Neutral)}



\section{Related Work Section 1}
\label{sec:related:sec1}

Graph Neural Networks have been around for quit a while and have proven very successful in modeling
of various graph-related tasks. Such tasks requie dealing with graph data which contains rich relation information among elements.

Therefore graph reasoning models, such as \ac
Lots of learning tasks require dealing with graph data which contains rich relation information among elements.
Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases
demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts
and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of
images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs)
are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In
recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph
recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this
survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.



\section{Related Work Section 2}
\label{sec:related:sec2}



\section{Related Work Section 3}
\label{sec:related:sec3}



\section{Conclusion}
\label{sec:related:conclusion}

